---
title: "SELF INTRODUCTION"
date: '2017-10-31T22:42:51-05:00'

output:
  html_document:
    df_print: paged
image: pic07.JPG
keywords: ''
slug: blog6
categories:
- ''
- ''
draft: no
---

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse) 
library(gapminder)  
library(here)
library(janitor)
```

## Practice using Markdown

Written assignments will be submitted using [Markdown](https://daringfireball.net/projects/markdown/). Markdown is a lightweight text formatting language that easily converts between file formats. It is integrated directly into [R Markdown](http://rmarkdown.rstudio.com/), which combines R code, output, and written text into a single document (`.Rmd`).

There is a very nice [Markdwown tutorial](https://commonmark.org/help/tutorial/) that I suggest you go through before working on your assignment. If you want to use a stand-alone Markdown editor [Typora](https://typora.io/) is a lightweight Markdown editor that inherently supports pandoc-flavored Markdown.

## Pandoc

[Pandoc](http://pandoc.org)is a program that converts Markdown files into basically anything else. It was created by [John MacFarlane](https://johnmacfarlane.net), a philosophy professor at the University of California, Berkeley and is widely used as a writing tool and as a basis for publishing workflow. Kieran Healy's [Plain Text Social Science workflow](http://plain-text.co) describes how to use Markdown and then convert your Markdown document to HTML, PDF, word, etc.

You should create a file whose name will be `your_name.Rmd`, so if I were submitting, my file would be called `Kostis_Christodoulou.Rmd`

# Task 1: Short biography written using markdown

#Heading 1
Hello, I am Peng CHEN [linkedin] (https://www.linkedin.com/in/peng-chen-b83187162/)and graduated from Finance, Accounting, and Management from the *University of Nottingham*. It is my best honor to take part in our **MFA 2022 programme** and can contribute to our programme and the whole *London Business School* community.

##Heading 2
I am a typical **Finance** person due to my continuous interest in Finance and refelcted from my previous internship **e.g.**:

* **Investment Analyst** in *CITIC PE*, *top 3 PE in CHINA*, and responsible for assisting the M&A event of the private hospitals. 
* **Analyst** in *CMS*ï¼Œ*top 5 IBD in CHINA*, and responsible for the dual dilligence for the IPO project.
* **Analyst** in *Yinhua Fund*, *top 5 Fund in CHINA*, and responsible for the TMT analysis, 



# Task 2: `gapminder` country comparison

Use the `glimpse` function and have a look at the first 20 rows of data in the `gapminder` dataset.

```{r}
glimpse(gapminder)

head(gapminder, 20) # look at the first 20 rows of the dataframe

```

I have created the `country_data` and `continent_data` with the code below.

```{r}
country_data <- gapminder %>% 
            filter(country == "China") 

continent_data <- gapminder %>% 
            filter(continent == "Asia")
```

First, create a plot of life expectancy over time for China. Map `year` on the x-axis, and `lifeExp` on the y-axis. Use `geom_point()` to see the actual data points and `geom_smooth(se = FALSE)` to plot the underlying trendlines. 

```{r, lifeExp_one_country}
plot1 <- ggplot(data = country_data, mapping = aes(x = year, y = lifeExp))+
   geom_point() +
   geom_smooth(se = FALSE) +
   NULL 

plot1
```

Next we need to add a title. Create a new plot, or extend plot1, using the `labs()` function to add an informative title to the plot.

```{r, lifeExp_one_country_with_label}
plot1<- plot1 +
  labs(title = "Trends in China's life expectancy from 1950s",
      x = "Year",
      y = "Life expenctancy") +
      NULL

plot1
```

Secondly, produce a plot for all countries in the *Asia* you come from. (Hint: map the `country` variable to the colour aesthetic. You also want to map `country` to the `group` aesthetic, so all points for each country are grouped together).

```{r lifeExp_one_continent}
ggplot(continent_data, mapping = aes(x = year, y = lifeExp , colour= country, group = country))+
  geom_point() + 
  geom_smooth(se = FALSE) +
  NULL
```

Finally, using the original `gapminder` data, produce a life expectancy over time graph, grouped (or faceted) by continent. We will remove all legends, adding the `theme(legend.position="none")` in the end of our ggplot.

```{r lifeExp_facet_by_continent}
ggplot(data = gapminder , mapping = aes(x = year , y = lifeExp , colour= continent))+
  geom_point() + 
  geom_smooth(se = FALSE) +
  facet_wrap(~continent) +
  theme(legend.position="none") + 
  NULL
```

Given these trends, what can you say about life expectancy since 1952? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

#World
1. As for the whole **World**, the life expectancy reflects the increasing trend from 1950 to 2000. Such circumstance can be explained by the more advanced healthcare technology, more healthy lifestyle, and more self-protection consciousness.

#Asia
1. For the continent from which I came, the life expectancy in nearly all the countries and districts increases the variance for the life expectancy becomes less than before, which may be due to the economic development and related improving medical service and infrastructure construciton. 

#China
1. In my own country, due to the more stable internal development situation and the stimulating economic policy, the national economic development causes the life expectancy to increase, regardless of some mild fluctuation. But after 1980, it increased with lower speed because of the saturation of the improvement of the medical service.

# Task 3: Brexit vote analysis

We will have a look at the results of the 2016 Brexit vote in the UK. First we read the data using `read_csv()` and have a quick glimpse at the data

```{r load_brexit_data, warning=FALSE, message=FALSE}
brexit_results <- read_csv(here::here("/Users/dp/Desktop/pre_programme_assignment/data","brexit_results.csv"))


glimpse(brexit_results)
```

To get a sense of the spread, or distribution, of the data, we can plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}

# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5) +
  labs(title = "Histogram of Brexit preference rate distribution",
    x = "preference rate for Brexit",
    y = "Count")

# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density() +
  labs(title = "Density plot of Brexit preference rate distribution",
    x = "preference rate for Brexit",
    y = "Density")


# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Empirical cumulative distribution of Brexit preference rate",
    x = "preference rate for Brexit",
    y = "Cumulative distribution")
  
```
One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`. To do this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select(leave_share, born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are positively correlated.

We can also create a scatterplot between these two variables using `geom_point`. We also add the best fit line, using `geom_smooth(method = "lm")`.

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  geom_smooth(method = "lm") + 
  theme_bw() +
  labs(title = "The relationship between proportion of native born residents and Brexit preference rate",
    x = "Proportion of native born residents",
    y = "Brexit preference rate") +
    NULL

```

You have the code for the plots, I would like you to revisit all of them and use the `labs()` function to add an informative title, subtitle, and axes titles to all plots.

What can you say about the relationship shown above? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

#Finding
It can be illustrated from the different graphs and data that there is high correlation between the proportion of native born residents and the preference rate for the Brexit. It can be explained that the native-born residents will stimulate the Brexit progress probably because of the internal competitive working environment and less working opportunities. Therefore, the fear mind of lossing the job causes them to opposite the immigration of European people under the EU UNION circumstance and to support the independent working situation.
> Type your answer after, and outside, this blockquote.

# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}

url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"

animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()


glimpse(animal_rescue)
```
One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use `group_by()... summarise()` or, simply [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}

# Method 1
animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())

# Method 2
animal_rescue %>% 
  count(cal_year, name="count")

```
Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))


animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))


```

Do you see anything strange in these tables? 

Finally, let us have a loot at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

There is two things we will do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.


Before we go on, however, we need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}

typeof(animal_rescue$incident_notional_cost)
animal_rescue <- animal_rescue %>% 
  mutate(incident_notional_cost = parse_number(incident_notional_cost))
typeof(animal_rescue$incident_notional_cost)

```


Now tht incident_notional_cost is numeric, let us quickly calculate summary statistics for each animal group. 


```{r, stats_on_incident_cost,message=FALSE, warning=FALSE}

animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  summarise(mean_incident_cost = mean (incident_notional_cost, na.rm=TRUE),
            median_incident_cost = median (incident_notional_cost, na.rm=TRUE),
            sd_incident_cost = sd (incident_notional_cost, na.rm=TRUE),
            min_incident_cost = min (incident_notional_cost, na.rm=TRUE),
            max_incident_cost = max (incident_notional_cost, na.rm=TRUE),
            count = n()) %>% 
  arrange(desc(mean_incident_cost))

```


Compare the mean and the median for each animal group. waht do you think this is telling us?
Anything else that stands out? Any outliers?

Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}

# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()

base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)



```

Which of these four graphs do you think best communicates the variability of the `incident_notional_cost` values? Also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.

From my viewpoint, compared with other graphs, the density graphy may be more accurate to illustrate the magnitude and the variability of the incident notional cost. And it will also show the central data from the grahh.

From the graph, I could conclude that the some species such as rabbit and ferret may have higher cost variance possibly due to the varied habitats. In addition, some species may have neeed higher cost (**higher magnitude on the graph**)probably becausee of the difficulty to rescue them. 

# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.